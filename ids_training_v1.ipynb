{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvVFcKywqi-2"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "! pip install pandas\n",
        "! pip3 install torch --index-url https://download.pytorch.org/whl/cu128\n",
        "! pip install ipdb\n",
        "! pip install tqdm\n",
        "! pip install pyarrow\n",
        "! pip install matplotlib\n",
        "! pip install scikit-learn\n",
        "! pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "X-MmhuY3qjvp",
        "outputId": "f140e01f-5d8c-4b69-b3c8-eccc82b9b4d1"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "import pandas as pd\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, IterableDataset\n",
        "import gc\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pyarrow.dataset as ds\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import json\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACFRdy1cqsto"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# ! cp drive/MyDrive/normalized-ids2018-parquet.tar.gz /content/\n",
        "! tar -xzvf normalized-ids2018-parquet.tar.gz normalized/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIwZWOqLKqTR"
      },
      "outputs": [],
      "source": [
        "PARQUET_FILES = glob.glob('normalized/*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhQA3e0nYgzh"
      },
      "outputs": [],
      "source": [
        "# canon columns\n",
        "CANON_COLUMN_INDEX = ['Fwd IAT Tot', 'Fwd Pkt Len Min', 'Down/Up Ratio', 'Dst Port', 'Fwd IAT Std', 'Fwd Header Len', 'Fwd IAT Min', 'Flow IAT Std', 'Active Std', 'Bwd IAT Max', 'Fwd Pkt Len Mean', 'Pkt Size Avg', 'PSH Flag Cnt', 'Flow IAT Mean', 'Fwd Act Data Pkts', 'Bwd Pkt Len Max', 'Flow IAT Max', 'ACK Flag Cnt', 'Bwd IAT Tot', 'Flow IAT Min', 'Bwd Pkts/b Avg', 'Fwd IAT Max', 'SYN Flag Cnt', 'Bwd Header Len', 'Fwd Seg Size Avg', 'Bwd Byts/b Avg', 'Subflow Bwd Byts', 'Pkt Len Max', 'Bwd Pkts/s', 'Fwd IAT Mean', 'Pkt Len Var', 'Fwd Pkt Len Std', 'Protocol', 'Init Bwd Win Byts', 'Active Min', 'Src Port', 'RST Flag Cnt', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Bwd Pkt Len Std', 'Fwd PSH Flags', 'Fwd Pkts/s', 'Bwd Blk Rate Avg', 'Flow Byts/s', 'CWE Flag Count', 'Pkt Len Std', 'Active Max', 'Fwd Byts/b Avg', 'Fwd Blk Rate Avg', 'URG Flag Cnt', 'Timestamp', 'Fwd Pkts/b Avg', 'Idle Mean', 'Idle Std', 'Fwd Pkt Len Max', 'Pkt Len Min', 'Flow Duration', 'Fwd Seg Size Min', 'Bwd IAT Min', 'TotLen Fwd Pkts', 'Flow Pkts/s', 'Active Mean', 'ECE Flag Cnt', 'Idle Min', 'Subflow Bwd Pkts', 'Bwd Pkt Len Mean', 'Pkt Len Mean', 'Tot Fwd Pkts', 'Bwd IAT Std', 'Bwd Seg Size Avg', 'Bwd URG Flags', 'Bwd Pkt Len Min', 'Tot Bwd Pkts', 'Subflow Fwd Pkts', 'Bwd IAT Mean', 'FIN Flag Cnt', 'Bwd PSH Flags', 'TotLen Bwd Pkts', 'Fwd URG Flags', 'Idle Max']\n",
        "CANON_COLUMN_INDEX.sort()\n",
        "CANON_COLUMN_INDEX.append('Label')\n",
        "print(CANON_COLUMN_INDEX)\n",
        "TRAINING_UNWANTED_COLUMNS = ['Timestamp', 'Flow ID', 'Dst IP', \"Src IP\"]\n",
        "TRAINING_WANTED_COLUMNS = []\n",
        "for col in CANON_COLUMN_INDEX:\n",
        "  if col not in TRAINING_UNWANTED_COLUMNS:\n",
        "    TRAINING_WANTED_COLUMNS.append(col)\n",
        "print(TRAINING_WANTED_COLUMNS)\n",
        "TRAINING_FEATURES = TRAINING_WANTED_COLUMNS[:-1]\n",
        "\n",
        "LENGTH = 16233002 # precalculated from data wrangling, see info.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFf3USxJshct"
      },
      "outputs": [],
      "source": [
        "# pyarrow parquet dataset\n",
        "class ArrowParquetDataset(IterableDataset):\n",
        "    def __init__(self, path, batch_size=1024, shuffle=True, splits=(0.7,0.1,0.2)):\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.splits = splits\n",
        "        self.mode(\"train\")\n",
        "\n",
        "        self.dataset = ds.dataset(self.path, format=\"parquet\")\n",
        "        self.scanner = self.dataset.scanner(columns=TRAINING_WANTED_COLUMNS, batch_size=self.batch_size)\n",
        "\n",
        "    def mode(self, m):\n",
        "        match m:\n",
        "            case \"train\":\n",
        "                self.mode_value = 0\n",
        "            case \"val\":\n",
        "                self.mode_value = 1\n",
        "            case \"test\":\n",
        "                self.mode_value = 2\n",
        "\n",
        "\n",
        "    # approximate iterations\n",
        "    def num_iterations(self):\n",
        "        return int((len(self) * self.splits[self.mode_value]) / self.batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.scanner.count_rows()\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = list(self.scanner.to_batches())\n",
        "\n",
        "        num_batches = len(batches)\n",
        "        train_end = int(num_batches * self.splits[0])\n",
        "        val_end = train_end + int(num_batches * self.splits[1])\n",
        "\n",
        "        match self.mode_value:\n",
        "            case 0:\n",
        "                batches = batches[:train_end]\n",
        "            case 1:\n",
        "                batches = batches[train_end:val_end]\n",
        "            case 2:\n",
        "                batches = batches[val_end:]\n",
        "\n",
        "        if self.shuffle:\n",
        "            random.shuffle(batches)\n",
        "\n",
        "        for batch in batches:\n",
        "            x = batch.select(TRAINING_FEATURES).to_tensor(null_to_nan=True)\n",
        "            y_string_array = batch.column(\"Label\")\n",
        "\n",
        "            # Optional: convert to torch.Tensor\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "            y = torch.tensor([(0.0 if val.as_py() == \"Benign\" else 1.0) for val in y_string_array], dtype=torch.float32)\n",
        "\n",
        "            # mask and impute nans\n",
        "            mask = torch.isnan(x).float()\n",
        "            x = torch.nan_to_num(x, nan=0.0)\n",
        "            x = torch.cat([x, mask], dim=1)\n",
        "\n",
        "            yield x, y\n",
        "\n",
        "DS_ARROW = ArrowParquetDataset(PARQUET_FILES, batch_size=1024)\n",
        "print(len(DS_ARROW))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKDGOEulhXME"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "class DNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_sizes, output_size):\n",
        "    super(DNN, self).__init__()\n",
        "\n",
        "    #layers\n",
        "    self.input = nn.Linear(input_size, hidden_sizes[0])\n",
        "    self.output = nn.Linear(hidden_sizes[-1], output_size)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.hiddens = nn.ModuleList()\n",
        "    for i in range(len(hidden_sizes) - 1):\n",
        "      self.hiddens.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.input(x))\n",
        "    x = self.dropout(x)\n",
        "    for layer in self.hiddens:\n",
        "      x = F.relu(layer(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.output(x)\n",
        "    return torch.sigmoid(x).view(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDhUdI5GtDIT"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "input_shape = len(TRAINING_FEATURES) * 2 # because masking\n",
        "model = DNN(input_shape, \n",
        "                [int(input_shape / 2), int(input_shape / 4)],\n",
        "                1)\n",
        "model_desc = model.__str__()\n",
        "print(model_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npQh_1W9f3I7"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model.to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "lr = 0.00001\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "weight_decay = 0.0001\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1,beta2), weight_decay=weight_decay)\n",
        "epochs = 10\n",
        "train_loss = -1\n",
        "\n",
        "historical_loss = []\n",
        "historical_val_loss = []\n",
        "\n",
        "train_start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0.0\n",
        "  epoch_val_loss = 0.0\n",
        "\n",
        "  model.train()\n",
        "  epoch_samples = 0\n",
        "  DS_ARROW.mode(\"train\")\n",
        "  for x, y in tqdm(DS_ARROW, total=DS_ARROW.num_iterations()):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    epoch_samples += y.size()[0]\n",
        "\n",
        "    optim.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = loss_fn(out, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  epoch_loss = epoch_loss / epoch_samples\n",
        "\n",
        "  model.eval()\n",
        "  epoch_val_samples = 0\n",
        "  DS_ARROW.mode(\"val\")\n",
        "  with torch.no_grad():\n",
        "    for x, y in tqdm(DS_ARROW, total=DS_ARROW.num_iterations()):\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      epoch_val_samples += y.size()[0]\n",
        "\n",
        "      out = model(x)\n",
        "      loss = loss_fn(out, y)\n",
        "\n",
        "      epoch_val_loss += loss.item()\n",
        "\n",
        "  epoch_val_loss = epoch_val_loss / epoch_val_samples\n",
        "  historical_val_loss.append(epoch_val_loss)\n",
        "  historical_loss.append(epoch_loss)\n",
        "\n",
        "  print(f'epoch: {epoch + 1}/{epochs}, train loss: {epoch_loss:.5f}, val loss: {epoch_val_loss:.5f}')\n",
        "  train_loss = epoch_loss\n",
        "min, sec = divmod(int(time.time() - train_start_time), 60)\n",
        "print(f'train time: {min}:{sec}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EHs6YqKrcE7"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_start_time = time.time()\n",
        "\n",
        "historical_label_tensor = []\n",
        "historical_pred_tensor = []\n",
        "\n",
        "DS_ARROW.mode(\"test\")\n",
        "epoch_samples = 0\n",
        "with torch.no_grad():\n",
        "  for x, y in tqdm(DS_ARROW, total=DS_ARROW.num_iterations()):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    epoch_samples += y.size()[0]\n",
        "\n",
        "    out = model(x)\n",
        "    test_loss += loss_fn(out, y).item()\n",
        "\n",
        "    if random.random() < 0.02:\n",
        "      historical_label_tensor.append(y.to(\"cpu\"))\n",
        "      historical_pred_tensor.append(out.to(\"cpu\"))\n",
        "\n",
        "test_loss = test_loss / epoch_samples\n",
        "print(f'test loss: {test_loss:.5f}')\n",
        "min, sec = divmod(int(time.time() - test_start_time), 60)\n",
        "print(f'test time: {min}:{sec}')\n",
        "print(f\"samples: {len(historical_label_tensor) * DS_ARROW.batch_size}/{epoch_samples} {(len(historical_label_tensor) * DS_ARROW.batch_size / epoch_samples) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analyze\n",
        "\n",
        "historical_pred = torch.cat(historical_pred_tensor).tolist()\n",
        "historical_label = torch.cat(historical_label_tensor).tolist()\n",
        "\n",
        "# epoch loss\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(historical_loss) + 1), historical_loss, marker='o', color='blue', label=\"training loss\")\n",
        "plt.plot(range(1, len(historical_val_loss) + 1), historical_val_loss, marker='o', color='orange', label=\"validation loss\")\n",
        "plt.title('Loss by Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracy\n",
        "thresh_steps = 100\n",
        "thresholds = torch.linspace(0, 1, steps=thresh_steps)\n",
        "accuracies = []\n",
        "for t in thresholds: \n",
        "    correct = 0\n",
        "    for i in range(len(historical_label)):\n",
        "        if (historical_pred[i] > t and historical_label[i] > 0.5) or \\\n",
        "        (historical_pred[i] < t and historical_label[i] < 0.5):\n",
        "            correct += 1\n",
        "\n",
        "    accuracies.append(correct / len(historical_label))\n",
        "\n",
        "thresholds = np.array(thresholds)\n",
        "accuracies = np.array(accuracies)\n",
        "\n",
        "maxima_accuracies, _ = find_peaks(accuracies)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(thresholds[1:], accuracies[1:], label='Accuracy')\n",
        "for i in maxima_accuracies:\n",
        "    x = thresholds[i]\n",
        "    y = accuracies[i]\n",
        "    plt.plot(x, y, 'ro')  # plot the point as a red dot\n",
        "    plt.text(x, y + 0.01, f\"({x:.3f}, {y:.3f})\")\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Threshold')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# roc and auc\n",
        "fpr_roc, tpr_roc, thresholds_roc = roc_curve(historical_label, historical_pred)\n",
        "roc_auc = auc(fpr_roc, tpr_roc)\n",
        "\n",
        "for i in range(len(thresholds_roc)):\n",
        "    if thresholds_roc[i] <= (best_thresh + (1/thresh_steps)) and \\\n",
        "    thresholds_roc[i] >= (best_thresh - (1/thresh_steps)):\n",
        "        best_fpr = fpr_roc[i]\n",
        "        best_tpr = tpr_roc[i]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr_roc, tpr_roc, color='darkorange', lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line\n",
        "plt.scatter([best_fpr], [best_tpr], color='red', zorder=5, label=f'Maxima')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# confusion matrix at best accuracy\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "tn = 0\n",
        "for i in range(len(historical_label)):\n",
        "    pred_benign = (historical_pred[i] <= best_thresh)\n",
        "    truth_benign = (historical_label[i] < 0.5)\n",
        "    if truth_benign:\n",
        "        if pred_benign:\n",
        "            tp += 1\n",
        "        else:\n",
        "            fn += 1\n",
        "    else:\n",
        "        if pred_benign:\n",
        "            fp += 1\n",
        "        else:\n",
        "            tn += 1\n",
        "tpr = tp / (tp + fn)\n",
        "fnr = fn / (tp + fn)\n",
        "tnr = tn / (tn + fp)\n",
        "fpr = fp / (tn + fp)\n",
        "\n",
        "conf_matrix = np.array([[tp, fp],[fn, tn]])\n",
        "classes = ['Benign', 'Intrusion']\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(conf_matrix, cmap='Blues')\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, conf_matrix[i, j], ha='center', va='center', color='black', fontsize=14)\n",
        "ax.set_xticks(np.arange(2))\n",
        "ax.set_yticks(np.arange(2))\n",
        "ax.set_xticklabels(['Actual ' + c for c in classes])\n",
        "ax.set_yticklabels(['Pred ' + c for c in classes])\n",
        "plt.title('Confusion Matrix', fontsize=16)\n",
        "plt.xlabel('Actual Label', fontsize=12)\n",
        "plt.ylabel('Predicted Label', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.colorbar(im)\n",
        "plt.show()\n",
        "\n",
        "print(f\"tpr: {tpr}, fnr: {fnr}, tnr: {tnr}, fpr: {fpr}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"path\": \"models/model_0.pth\",\n",
            "    \"lr\": 1e-05,\n",
            "    \"beta1\": 0.9,\n",
            "    \"beta2\": 0.999,\n",
            "    \"weight_decay\": 0.0001,\n",
            "    \"epochs\": 10,\n",
            "    \"train_loss\": 6e-05,\n",
            "    \"val_loss\": 0.00011,\n",
            "    \"test_loss\": 0.0008948093706207522,\n",
            "    \"acc\": 0.9029998779296875,\n",
            "    \"thresh\": 0.9898989796638489,\n",
            "    \"auc\": 0.7177023562327813,\n",
            "    \"desc\": \"DNN(\\n  (input): Linear(in_features=158, out_features=79, bias=True)\\n  (output): Linear(in_features=39, out_features=1, bias=True)\\n  (dropout): Dropout(p=0.2, inplace=False)\\n  (hiddens): ModuleList(\\n    (0): Linear(in_features=79, out_features=39, bias=True)\\n  )\\n)\",\n",
            "    \"confusion\": {\n",
            "        \"tp\": 59179,\n",
            "        \"fp\": 6357,\n",
            "        \"fn\": 0,\n",
            "        \"tn\": 0,\n",
            "        \"tpr\": 1.0,\n",
            "        \"fnr\": 0.0,\n",
            "        \"tnr\": 0.0,\n",
            "        \"fpr\": 1.0\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# review\n",
        "with open(\"models/models.json\", \"r\") as file:\n",
        "    saved_models = json.load(file)\n",
        "save_path = f\"models/model_{len(saved_models)}.pth\"\n",
        "\n",
        "model_object = {\n",
        "    \"path\": save_path,\n",
        "    \"lr\": float(lr),\n",
        "    \"beta1\": float(beta1),\n",
        "    \"beta2\": float(beta2),\n",
        "    \"weight_decay\": float(weight_decay),\n",
        "    \"epochs\": int(epochs),\n",
        "    \"train_loss\": float(0.00006),\n",
        "    \"val_loss\": float(0.00011),\n",
        "    \"test_loss\": float(test_loss),\n",
        "    \"acc\": float(best_acc),\n",
        "    \"thresh\": float(best_thresh),\n",
        "    \"auc\": float(roc_auc),\n",
        "    \"desc\": str(model_desc),\n",
        "    \"confusion\": {\n",
        "        \"tp\": int(tp),\n",
        "        \"fp\": int(fp),\n",
        "        \"fn\": int(fn),\n",
        "        \"tn\": int(tn),\n",
        "        \"tpr\": float(tpr),\n",
        "        \"fnr\": float(fnr),\n",
        "        \"tnr\": float(tnr),\n",
        "        \"fpr\": float(fpr),\n",
        "    },\n",
        "}\n",
        "json_str = json.dumps(model_object, indent=4)\n",
        "\n",
        "print(json_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m save = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msave (y/n): \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save == \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtorch\u001b[49m.save(model.state_dict(), save_path)\n\u001b[32m      5\u001b[39m     notes = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnotes: \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     model_object[\u001b[33m'\u001b[39m\u001b[33mnotes\u001b[39m\u001b[33m'\u001b[39m] = notes\n",
            "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# save\n",
        "save = input(\"save (y/n): \")\n",
        "if save == \"y\":\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    notes = input(\"notes: \")\n",
        "    model_object['notes'] = notes\n",
        "    with open(\"models/models.json\", \"r\") as file:\n",
        "        saved_models = json.load(file)\n",
        "        saved_models.append(model_object)\n",
        "    with open(\"models/models.json\", \"w\") as file:\n",
        "        json.dump(saved_models, file, indent=4)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
