{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvVFcKywqi-2"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "! pip install pandas\n",
        "! pip3 install torch --index-url https://download.pytorch.org/whl/cu128\n",
        "! pip install ipdb\n",
        "! pip install tqdm\n",
        "! pip install pyarrow\n",
        "! pip install matplotlib\n",
        "! pip install scikit-learn\n",
        "! pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "X-MmhuY3qjvp",
        "outputId": "f140e01f-5d8c-4b69-b3c8-eccc82b9b4d1"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "import pandas as pd\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, IterableDataset\n",
        "import gc\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pyarrow.dataset as ds\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import json\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACFRdy1cqsto"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# ! cp drive/MyDrive/normalized-ids2018-parquet.tar.gz /content/\n",
        "# ! tar -xzvf normalized-ids2018-parquet.tar.gz normalized/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIwZWOqLKqTR"
      },
      "outputs": [],
      "source": [
        "PARQUET_FILES = glob.glob('normalized/*.parquet')\n",
        "PARQUET_FILES_BENIGN = glob.glob('normalized-benign/*.parquet')\n",
        "print(PARQUET_FILES)\n",
        "print(PARQUET_FILES_BENIGN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhQA3e0nYgzh"
      },
      "outputs": [],
      "source": [
        "# canon columns\n",
        "CANON_COLUMN_INDEX = ['Fwd IAT Tot', 'Fwd Pkt Len Min', 'Down/Up Ratio', 'Dst Port', 'Fwd IAT Std', 'Fwd Header Len', 'Fwd IAT Min', 'Flow IAT Std', 'Active Std', 'Bwd IAT Max', 'Fwd Pkt Len Mean', 'Pkt Size Avg', 'PSH Flag Cnt', 'Flow IAT Mean', 'Fwd Act Data Pkts', 'Bwd Pkt Len Max', 'Flow IAT Max', 'ACK Flag Cnt', 'Bwd IAT Tot', 'Flow IAT Min', 'Bwd Pkts/b Avg', 'Fwd IAT Max', 'SYN Flag Cnt', 'Bwd Header Len', 'Fwd Seg Size Avg', 'Bwd Byts/b Avg', 'Subflow Bwd Byts', 'Pkt Len Max', 'Bwd Pkts/s', 'Fwd IAT Mean', 'Pkt Len Var', 'Fwd Pkt Len Std', 'Protocol', 'Init Bwd Win Byts', 'Active Min', 'Src Port', 'RST Flag Cnt', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Bwd Pkt Len Std', 'Fwd PSH Flags', 'Fwd Pkts/s', 'Bwd Blk Rate Avg', 'Flow Byts/s', 'CWE Flag Count', 'Pkt Len Std', 'Active Max', 'Fwd Byts/b Avg', 'Fwd Blk Rate Avg', 'URG Flag Cnt', 'Timestamp', 'Fwd Pkts/b Avg', 'Idle Mean', 'Idle Std', 'Fwd Pkt Len Max', 'Pkt Len Min', 'Flow Duration', 'Fwd Seg Size Min', 'Bwd IAT Min', 'TotLen Fwd Pkts', 'Flow Pkts/s', 'Active Mean', 'ECE Flag Cnt', 'Idle Min', 'Subflow Bwd Pkts', 'Bwd Pkt Len Mean', 'Pkt Len Mean', 'Tot Fwd Pkts', 'Bwd IAT Std', 'Bwd Seg Size Avg', 'Bwd URG Flags', 'Bwd Pkt Len Min', 'Tot Bwd Pkts', 'Subflow Fwd Pkts', 'Bwd IAT Mean', 'FIN Flag Cnt', 'Bwd PSH Flags', 'TotLen Bwd Pkts', 'Fwd URG Flags', 'Idle Max']\n",
        "CANON_COLUMN_INDEX.sort()\n",
        "CANON_COLUMN_INDEX.append('Label')\n",
        "print(CANON_COLUMN_INDEX)\n",
        "TRAINING_UNWANTED_COLUMNS = ['Timestamp', 'Flow ID', 'Dst IP', \"Src IP\"]\n",
        "TRAINING_WANTED_COLUMNS = []\n",
        "for col in CANON_COLUMN_INDEX:\n",
        "  if col not in TRAINING_UNWANTED_COLUMNS:\n",
        "    TRAINING_WANTED_COLUMNS.append(col)\n",
        "print(TRAINING_WANTED_COLUMNS)\n",
        "TRAINING_FEATURES = TRAINING_WANTED_COLUMNS[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFf3USxJshct"
      },
      "outputs": [],
      "source": [
        "# pyarrow parquet dataset\n",
        "class ArrowParquetDataset(IterableDataset):\n",
        "    def __init__(self, path, batch_size=1024, shuffle=True, splits=[0.7,0.1,0.2]):\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.splits = splits\n",
        "        self.mode(\"train\")\n",
        "\n",
        "        self.dataset = ds.dataset(self.path, format=\"parquet\")\n",
        "        self.scanner = self.dataset.scanner(columns=TRAINING_WANTED_COLUMNS, batch_size=self.batch_size)\n",
        "\n",
        "    def mode(self, m):\n",
        "        self.mode_value = m\n",
        "\n",
        "    # approximate iterations\n",
        "    def num_iterations(self):\n",
        "        return int((len(self) * self.splits[self.mode_value]) / self.batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.scanner.count_rows()\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = list(self.scanner.to_batches())\n",
        "\n",
        "        num_batches = len(batches)\n",
        "        batch_intervals = [0]\n",
        "        running_total = 0\n",
        "        for i in range(len(self.splits) - 1):\n",
        "            length = int(np.floor(num_batches * self.splits[i]))\n",
        "            batch_intervals.append(length + running_total)\n",
        "            running_total += length\n",
        "        batch_intervals.append(num_batches)\n",
        "\n",
        "        batches = batches[batch_intervals[self.mode_value]:batch_intervals[self.mode_value + 1]]\n",
        "\n",
        "        if self.shuffle:\n",
        "            random.shuffle(batches)\n",
        "\n",
        "        for batch in batches:\n",
        "            x = batch.select(TRAINING_FEATURES).to_tensor(null_to_nan=True)\n",
        "            \n",
        "            y_string_array = batch.column(\"Label\")\n",
        "            y = torch.tensor([(0.0 if val.as_py() == \"Benign\" else 1.0) for val in y_string_array], dtype=torch.float32)\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "            # in val and train modes ignore intrusions\n",
        "            if (self.mode_value == 0 or self.mode_value == 1):\n",
        "                benign_mask = (y == 0.0)\n",
        "                x = x[benign_mask]\n",
        "                y = y[benign_mask]\n",
        "            \n",
        "            # mask and impute nans\n",
        "            mask = torch.isnan(x).float()\n",
        "            x = torch.nan_to_num(x, nan=0.0)\n",
        "            x = torch.cat([x, mask], dim=1)\n",
        "            \n",
        "            yield x, y\n",
        "\n",
        "DS_ARROW = ArrowParquetDataset(PARQUET_FILES, batch_size=512, splits=[0.6,0.1,0.1,0.2])\n",
        "print(len(DS_ARROW))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKDGOEulhXME"
      },
      "outputs": [],
      "source": [
        "# dnn model\n",
        "class DNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_sizes, output_size):\n",
        "    super(DNN, self).__init__()\n",
        "\n",
        "    #layers\n",
        "    self.input = nn.Linear(input_size, hidden_sizes[0])\n",
        "    self.output = nn.Linear(hidden_sizes[-1], output_size)\n",
        "    self.dropout = nn.Dropout(0.6)\n",
        "    self.hiddens = nn.ModuleList()\n",
        "    for i in range(len(hidden_sizes) - 1):\n",
        "      self.hiddens.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.input(x))\n",
        "    for layer in self.hiddens:\n",
        "      x = self.dropout(x)\n",
        "      x = F.relu(layer(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.output(x)\n",
        "    # return torch.sigmoid(x).view(-1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDhUdI5GtDIT"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "input_shape = len(TRAINING_FEATURES) * 2 # because masking\n",
        "\n",
        "# autoencoder\n",
        "model = DNN(\n",
        "    input_shape,\n",
        "    [int(input_shape / 2), int(input_shape / 2)],\n",
        "    input_shape\n",
        ")\n",
        "\n",
        "model_desc = model.__str__()\n",
        "print(model_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npQh_1W9f3I7"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model.to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn_no_reduction = nn.MSELoss(reduction='none')\n",
        "lr = 0.00001\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "weight_decay = 0.001\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1,beta2), weight_decay=weight_decay)\n",
        "epochs = 20\n",
        "train_loss = -1\n",
        "\n",
        "reconstruction_l2 = 0.0\n",
        "\n",
        "historical_loss = []\n",
        "historical_val_loss = []\n",
        "historical_reconstruction = []\n",
        "\n",
        "train_start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0.0\n",
        "  epoch_val_loss = 0.0\n",
        "\n",
        "  model.train()\n",
        "  epoch_samples = 0\n",
        "  DS_ARROW.mode(0)\n",
        "  for x, _ in tqdm(DS_ARROW, total=DS_ARROW.num_iterations()):\n",
        "    x = x.to(device)\n",
        "    batch_size = x.size()[0]\n",
        "    if batch_size == 0: continue\n",
        "    epoch_samples += batch_size\n",
        "\n",
        "    optim.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = loss_fn(out, x)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    epoch_loss += loss.item() * batch_size\n",
        "\n",
        "  epoch_loss = epoch_loss / epoch_samples\n",
        "\n",
        "  epoch_l2s = []\n",
        "\n",
        "  model.eval()\n",
        "  epoch_val_samples = 0\n",
        "  DS_ARROW.mode(1)\n",
        "  with torch.no_grad():\n",
        "    for x, _ in tqdm(DS_ARROW, total=DS_ARROW.num_iterations()):\n",
        "      x = x.to(device)\n",
        "      batch_size = x.size()[0]\n",
        "      if batch_size == 0: continue\n",
        "      epoch_val_samples += batch_size\n",
        "\n",
        "      out = model(x)\n",
        "      loss = loss_fn(out, x)\n",
        "      epoch_val_loss += loss.item() * batch_size\n",
        "\n",
        "  epoch_val_loss = epoch_val_loss / epoch_val_samples\n",
        "  historical_val_loss.append(epoch_val_loss)\n",
        "  historical_loss.append(epoch_loss)\n",
        "\n",
        "  print(f'epoch: {epoch + 1}/{epochs}, train loss: {epoch_loss:.5f}, val loss: {epoch_val_loss:.5f}')\n",
        "  train_loss = epoch_loss\n",
        "min, sec = divmod(int(time.time() - train_start_time), 60)\n",
        "print(f'train time: {min}:{sec}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        print(f\"{name}: original weight: {torch.sum(module.weight == 0).item()}\")\n",
        "        module = prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "        prune.remove(module, 'weight')\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        print(f\"{name}: Pruned weights: {torch.sum(module.weight == 0).item()} zeros\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# threshold search\n",
        "search_start = 10\n",
        "search_depth = 4\n",
        "search_length = search_depth * 20\n",
        "search_counter = 0\n",
        "\n",
        "best_fscore = np.array([-1,-1])\n",
        "best_fscore_sum = -1\n",
        "best_threshold = -1\n",
        "best_percision = -1\n",
        "best_recall = -1\n",
        "\n",
        "historical_threshold = []\n",
        "historical_fscore = []\n",
        "\n",
        "threshold_search_start_time = time.time()\n",
        "for i in range(search_depth):\n",
        "    search_step = 10 ** (i * -1)\n",
        "    for j in range(-9,10):\n",
        "        search_counter += 1\n",
        "        threshold = j * search_step + search_start\n",
        "        y_pred_tensor = []\n",
        "        y_true_tensor = []\n",
        "\n",
        "        DS_ARROW.mode(2)\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(DS_ARROW, total=DS_ARROW.num_iterations()):\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "\n",
        "                out = model(x)\n",
        "                reconstruction_losses = loss_fn_no_reduction(out, x)\n",
        "                l2_dists = torch.norm(reconstruction_losses, p=2, dim=1)\n",
        "                thresholded = (l2_dists > threshold).float()\n",
        "                y_pred_tensor.append(thresholded.bool())\n",
        "                y_true_tensor.append(y.bool())\n",
        "        \n",
        "        y_pred = torch.cat(y_pred_tensor).tolist()\n",
        "        y_true = torch.cat(y_true_tensor).tolist()\n",
        "\n",
        "        percision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred)\n",
        "        curr_weighted_sum = fscore[0] + 1 * fscore[1]\n",
        "        historical_fscore.append(curr_weighted_sum)\n",
        "        historical_threshold.append(threshold)\n",
        "        print(f\"search: {search_counter}/{search_length}, threshold: {threshold}, percision: {percision}, recall: {recall}, fscore: {fscore}, fscore weighted sum: {curr_weighted_sum}\")\n",
        "        if best_fscore_sum < curr_weighted_sum:\n",
        "            best_threshold = threshold\n",
        "            best_fscore_sum = curr_weighted_sum\n",
        "            best_fscore = fscore\n",
        "            best_percision = percision\n",
        "            best_recall = recall\n",
        "    \n",
        "    search_start = best_threshold\n",
        "\n",
        "print(f\"best threshold: {best_threshold}, percision: {best_percision}, recall: {best_recall}, fscore: {best_fscore}, fscore weighted sum: {best_fscore_sum}\")\n",
        "min, sec = divmod(int(time.time() - threshold_search_start_time), 60)\n",
        "print(f'train time: {min}:{sec}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EHs6YqKrcE7"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_start_time = time.time()\n",
        "\n",
        "historical_label_tensor = []\n",
        "historical_pred_tensor = []\n",
        "\n",
        "DS_ARROW.mode(3)\n",
        "epoch_samples = 0\n",
        "with torch.no_grad():\n",
        "  for x, y in tqdm(DS_ARROW, total=DS_ARROW.num_iterations()):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    batch_size = x.size()[0]\n",
        "    epoch_samples += batch_size\n",
        "\n",
        "    out = model(x)\n",
        "    reconstruction_losses = loss_fn_no_reduction(out, x)\n",
        "    l2_dists = torch.norm(reconstruction_losses, p=2, dim=1)\n",
        "    thresholded = (l2_dists > best_threshold).float()\n",
        "    test_loss += torch.isclose(thresholded, y).float().sum().item()\n",
        "\n",
        "    historical_label_tensor.append(y.to(\"cpu\"))\n",
        "    historical_pred_tensor.append(thresholded.to(\"cpu\"))\n",
        "\n",
        "test_loss = test_loss / epoch_samples\n",
        "print(f'test loss (accuracy): {test_loss:.5f}')\n",
        "\n",
        "historical_pred = torch.cat(historical_pred_tensor).tolist()\n",
        "historical_label = torch.cat(historical_label_tensor).tolist()\n",
        "test_percision, test_recall, test_fscore, test_support = precision_recall_fscore_support(y_true, y_pred)\n",
        "print(f\"percision: {test_percision}, recall: {test_recall}, fscore: {test_fscore}\\nsupport: {test_support}\")\n",
        "\n",
        "min, sec = divmod(int(time.time() - test_start_time), 60)\n",
        "print(f'test time: {min}:{sec}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analyze\n",
        "\n",
        "with open(\"models/models.json\", \"r\") as file:\n",
        "    saved_models = json.load(file)\n",
        "model_name = f'model_{len(saved_models)}'\n",
        "color_list = ['red', 'orange', 'yellow', 'green', 'blue', 'purple']\n",
        "\n",
        "historical_pred = torch.cat(historical_pred_tensor).tolist()\n",
        "historical_label = torch.cat(historical_label_tensor).tolist()\n",
        "\n",
        "# epoch loss\n",
        "loss_plt, ax = plt.subplots()\n",
        "ax.plot(range(1, len(historical_loss) + 1), historical_loss, marker='o', color='blue', label=\"training loss\")\n",
        "ax.plot(range(1, len(historical_val_loss) + 1), historical_val_loss, marker='o', color='orange', label=\"validation loss\")\n",
        "ax.set_title(f'{model_name}: Loss by Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.legend()\n",
        "plt.show(loss_plt.number)\n",
        "\n",
        "# threshold search\n",
        "threshold_plt, ax = plt.subplots()\n",
        "thresh_sort, fscore_sort = zip(*sorted(zip(historical_threshold, historical_fscore)))\n",
        "ax.plot(thresh_sort, fscore_sort, color='blue')\n",
        "ax.plot(best_threshold, best_fscore_sum, color='red', marker='o', label=f'chosen threshold={best_threshold}')\n",
        "ax.set_title(f'{model_name}: threshold search')\n",
        "ax.set_xlabel('threshold')\n",
        "ax.set_ylabel('fscore')\n",
        "ax.legend()\n",
        "plt.show(threshold_plt.number)\n",
        "\n",
        "# confusion matrix\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "tn = 0\n",
        "for i in range(len(historical_label)):\n",
        "    if historical_label[i]:\n",
        "        if historical_pred[i]:\n",
        "            tp += 1\n",
        "        else:\n",
        "            fn += 1\n",
        "    else:\n",
        "        if historical_pred[i]:\n",
        "            fp += 1\n",
        "        else:\n",
        "            tn += 1\n",
        "tpr = tp / (tp + fn)\n",
        "fpr = fp / (tn + fp)\n",
        "tnr = tn / (tn + fp)\n",
        "fnr = fn / (tp + fn)\n",
        "\n",
        "conf_matrix = np.array([[tp, fp],[fn, tn]])\n",
        "classes = ['Intrusion (positive)','Benign (negative)']\n",
        "conf_plt, ax = plt.subplots()\n",
        "im = ax.imshow(conf_matrix, cmap='Blues')\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, conf_matrix[i, j], ha='center', va='center', color='black', fontsize=14)\n",
        "ax.set_xticks(np.arange(2))\n",
        "ax.set_yticks(np.arange(2))\n",
        "ax.set_xticklabels(['Actual ' + c for c in classes])\n",
        "ax.set_yticklabels(['Pred ' + c for c in classes])\n",
        "ax.set_title('Confusion Matrix', fontsize=16)\n",
        "ax.set_xlabel('Actual Label', fontsize=12)\n",
        "ax.set_ylabel('Predicted Label', fontsize=12)\n",
        "conf_plt.tight_layout()\n",
        "conf_plt.colorbar(im)\n",
        "plt.show(conf_plt.number)\n",
        "print(f\"tpr: {tpr}, fpr: {fpr}, tnr: {fnr}, fnr: {fnr}\")\n",
        "\n",
        "# chart, name\n",
        "charts = [(loss_plt, f\"charts/{model_name}-loss.jpg\"), \n",
        "          (conf_plt, f\"charts/{model_name}-conf.jpg\"),\n",
        "          (threshold_plt, f\"charts/{model_name}-threshold.jpg\"),\n",
        "         ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# review\n",
        "save_path = f\"models/{model_name}.pth\"\n",
        "model_object = {\n",
        "    \"path\": save_path,\n",
        "    \"lr\": float(lr),\n",
        "    \"beta1\": float(beta1),\n",
        "    \"beta2\": float(beta2),\n",
        "    \"weight_decay\": float(weight_decay),\n",
        "    \"epochs\": int(epochs),\n",
        "    \"loss_fn\": str(loss_fn.__str__()),\n",
        "    \"train_loss\": float(train_loss),\n",
        "    \"val_loss\": float(epoch_val_loss),\n",
        "    \"test_loss\": float(test_loss),\n",
        "    \"desc\": str(model_desc),\n",
        "    \"acc\": float(test_loss),\n",
        "    \"thresh\": float(best_threshold),\n",
        "    \"percision\": list(test_percision),\n",
        "    \"recall\": list(test_recall),\n",
        "    \"fscore\": list(test_fscore),\n",
        "    \"tpr\": float(tpr),\n",
        "    \"fpr\": float(fpr),\n",
        "    \"tnr\": float(tnr),\n",
        "    \"fnr\": float(fnr),\n",
        "    \"charts\": [name for _, name in charts],\n",
        "}\n",
        "json_str = json.dumps(model_object, indent=4)\n",
        "\n",
        "print(json_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save\n",
        "save = input(\"save (y/n): \")\n",
        "if save == \"y\":\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    notes = input(\"notes: \")\n",
        "    model_object['notes'] = notes\n",
        "\n",
        "    for chart, name in charts:\n",
        "        chart.savefig(name)\n",
        "\n",
        "    with open(\"models/models.json\", \"r\") as file:\n",
        "        saved_models = json.load(file)\n",
        "        saved_models.append(model_object)\n",
        "    with open(\"models/models.json\", \"w\") as file:\n",
        "        json.dump(saved_models, file, indent=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
