{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjLeMHLmKNbe",
        "outputId": "e19a6352-3cc5-4dba-fe57-b80569fee763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The process cannot access the file because it is being used by another process.\n",
            "The process cannot access the file because it is being used by another process.\n",
            "The process cannot access the file because it is being used by another process.\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# install pip dependencies\n",
        "! pip install -q kaggle\n",
        "! pip install pandas\n",
        "! pip install torch datasets\n",
        "! pip install ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07SmGsskMtoR",
        "outputId": "f05a05b2-9e9b-41b9-fe1e-678523c3708a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# kaggle authentication\n",
        "# upload kaggle.json\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMlU5RnIRD-S"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import io\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akOIb_RFM6as",
        "outputId": "cfd987c1-457f-44d0-890d-0f82a06d1290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/kaggle/input/ids-intrusion-csv/02-28-2018.csv', '/kaggle/input/ids-intrusion-csv/03-01-2018.csv', '/kaggle/input/ids-intrusion-csv/02-16-2018.csv', '/kaggle/input/ids-intrusion-csv/02-15-2018.csv', '/kaggle/input/ids-intrusion-csv/02-21-2018.csv', '/kaggle/input/ids-intrusion-csv/03-02-2018.csv', '/kaggle/input/ids-intrusion-csv/02-22-2018.csv', '/kaggle/input/ids-intrusion-csv/02-20-2018.csv', '/kaggle/input/ids-intrusion-csv/02-14-2018.csv', '/kaggle/input/ids-intrusion-csv/02-23-2018.csv']\n"
          ]
        }
      ],
      "source": [
        "# download data to paraquet\n",
        "download_path = kagglehub.dataset_download(\"solarmainframe/ids-intrusion-csv\")\n",
        "raw_csv_files = glob.glob(os.path.join(download_path, \"*.csv\"))\n",
        "print(raw_csv_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUUBuev_gVVW",
        "outputId": "e24deddd-adf9-4ed5-9106-27c3b84ccc1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "splitting: 1/10\n",
            "splitting: 2/10\n",
            "splitting: 3/10\n",
            "splitting: 4/10\n",
            "splitting: 5/10\n",
            "splitting: 6/10\n",
            "splitting: 7/10\n",
            "splitting: 8/10\n",
            "splitting: 9/10\n",
            "splitting: 10/10\n"
          ]
        }
      ],
      "source": [
        "# split csvs into managable chunks\n",
        "\n",
        "CSV_FILES = []\n",
        "\n",
        "num_raw_files = len(raw_csv_files)\n",
        "for i, file in enumerate(raw_csv_files):\n",
        "  print(f\"splitting: {i+1}/{num_raw_files}\")\n",
        "  filename = os.path.splitext(os.path.basename(file))[0]\n",
        "  max_bytes = 500 * 1024 * 1024\n",
        "\n",
        "  part_idx = 1\n",
        "  current_size = 0\n",
        "\n",
        "  with open(file, 'r') as infile:\n",
        "    header = infile.readline()\n",
        "    lines_buffer = []\n",
        "    for line in infile:\n",
        "      lines_buffer.append(line)\n",
        "      current_size += len(line.encode('utf-8'))  # size in bytes\n",
        "\n",
        "      # If current chunk exceeds the limit, write to file\n",
        "      if current_size >= max_bytes:\n",
        "        out_path = os.path.join(f\"/content/{filename}_part{part_idx}.csv\")\n",
        "        with open(out_path, 'w') as outfile:\n",
        "          outfile.write(header)\n",
        "          outfile.writelines(lines_buffer)\n",
        "        CSV_FILES.append(out_path)\n",
        "        part_idx += 1\n",
        "        lines_buffer = []\n",
        "        current_size = 0\n",
        "\n",
        "    # Write remaining lines\n",
        "    if lines_buffer:\n",
        "      out_path = os.path.join(f\"/content/{filename}_part{part_idx}.csv\")\n",
        "      with open(out_path, 'w') as outfile:\n",
        "        outfile.write(header)\n",
        "        outfile.writelines(lines_buffer)\n",
        "      CSV_FILES.append(out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbFsdVa4B-Ho",
        "outputId": "300c1fce-97ab-4348-ae40-b00a7c98f6f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-6-1566101961.py:1: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  test_df = pd.read_csv(CSV_FILES[1])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n",
            "       'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
            "       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
            "       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
            "       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
            "       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
            "       'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
            "       'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
            "       'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
            "       'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
            "       'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
            "       'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
            "       'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
            "       'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
            "       'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
            "       'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
            "       'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
            "       'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
            "       'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
            "       'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
            "       'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df = pd.read_csv(CSV_FILES[1])\n",
        "NUM_COLUMNS = len(test_df.columns)\n",
        "print(test_df.columns)\n",
        "del test_df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXLhGrO8jMXT",
        "outputId": "68828384-8f79-4a9d-b82e-d6845a40bd0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ACK Flag Cnt', 'Active Max', 'Active Mean', 'Active Min', 'Active Std', 'Bwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Header Len', 'Bwd IAT Max', 'Bwd IAT Mean', 'Bwd IAT Min', 'Bwd IAT Std', 'Bwd IAT Tot', 'Bwd PSH Flags', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Min', 'Bwd Pkt Len Std', 'Bwd Pkts/b Avg', 'Bwd Pkts/s', 'Bwd Seg Size Avg', 'Bwd URG Flags', 'CWE Flag Count', 'Down/Up Ratio', 'Dst Port', 'ECE Flag Cnt', 'FIN Flag Cnt', 'Flow Byts/s', 'Flow Duration', 'Flow IAT Max', 'Flow IAT Mean', 'Flow IAT Min', 'Flow IAT Std', 'Flow Pkts/s', 'Fwd Act Data Pkts', 'Fwd Blk Rate Avg', 'Fwd Byts/b Avg', 'Fwd Header Len', 'Fwd IAT Max', 'Fwd IAT Mean', 'Fwd IAT Min', 'Fwd IAT Std', 'Fwd IAT Tot', 'Fwd PSH Flags', 'Fwd Pkt Len Max', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Min', 'Fwd Pkt Len Std', 'Fwd Pkts/b Avg', 'Fwd Pkts/s', 'Fwd Seg Size Avg', 'Fwd Seg Size Min', 'Fwd URG Flags', 'Idle Max', 'Idle Mean', 'Idle Min', 'Idle Std', 'Init Bwd Win Byts', 'Init Fwd Win Byts', 'PSH Flag Cnt', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Min', 'Pkt Len Std', 'Pkt Len Var', 'Pkt Size Avg', 'Protocol', 'RST Flag Cnt', 'SYN Flag Cnt', 'Src Port', 'Subflow Bwd Byts', 'Subflow Bwd Pkts', 'Subflow Fwd Byts', 'Subflow Fwd Pkts', 'Timestamp', 'Tot Bwd Pkts', 'Tot Fwd Pkts', 'TotLen Bwd Pkts', 'TotLen Fwd Pkts', 'URG Flag Cnt', 'Label']\n"
          ]
        }
      ],
      "source": [
        "# canon columns\n",
        "CANON_COLUMN_INDEX = ['Fwd IAT Tot', 'Fwd Pkt Len Min', 'Down/Up Ratio', 'Dst Port', 'Fwd IAT Std', 'Fwd Header Len', 'Fwd IAT Min', 'Flow IAT Std', 'Active Std', 'Bwd IAT Max', 'Fwd Pkt Len Mean', 'Pkt Size Avg', 'PSH Flag Cnt', 'Flow IAT Mean', 'Fwd Act Data Pkts', 'Bwd Pkt Len Max', 'Flow IAT Max', 'ACK Flag Cnt', 'Bwd IAT Tot', 'Flow IAT Min', 'Bwd Pkts/b Avg', 'Fwd IAT Max', 'SYN Flag Cnt', 'Bwd Header Len', 'Fwd Seg Size Avg', 'Bwd Byts/b Avg', 'Subflow Bwd Byts', 'Pkt Len Max', 'Bwd Pkts/s', 'Fwd IAT Mean', 'Pkt Len Var', 'Fwd Pkt Len Std', 'Protocol', 'Init Bwd Win Byts', 'Active Min', 'Src Port', 'RST Flag Cnt', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Bwd Pkt Len Std', 'Fwd PSH Flags', 'Fwd Pkts/s', 'Bwd Blk Rate Avg', 'Flow Byts/s', 'CWE Flag Count', 'Pkt Len Std', 'Active Max', 'Fwd Byts/b Avg', 'Fwd Blk Rate Avg', 'URG Flag Cnt', 'Timestamp', 'Fwd Pkts/b Avg', 'Idle Mean', 'Idle Std', 'Fwd Pkt Len Max', 'Pkt Len Min', 'Flow Duration', 'Fwd Seg Size Min', 'Bwd IAT Min', 'TotLen Fwd Pkts', 'Flow Pkts/s', 'Active Mean', 'ECE Flag Cnt', 'Idle Min', 'Subflow Bwd Pkts', 'Bwd Pkt Len Mean', 'Pkt Len Mean', 'Tot Fwd Pkts', 'Bwd IAT Std', 'Bwd Seg Size Avg', 'Bwd URG Flags', 'Bwd Pkt Len Min', 'Tot Bwd Pkts', 'Subflow Fwd Pkts', 'Bwd IAT Mean', 'FIN Flag Cnt', 'Bwd PSH Flags', 'TotLen Bwd Pkts', 'Fwd URG Flags', 'Idle Max']\n",
        "CANON_COLUMN_INDEX.sort()\n",
        "CANON_COLUMN_INDEX.append('Label')\n",
        "print(CANON_COLUMN_INDEX)\n",
        "TRAINING_UNWANTED_COLUMNS = ['Timestamp', 'Flow ID', 'Dst IP', \"Src IP\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCteS2b1HwyZ",
        "outputId": "e026d55f-d94a-4572-e2b3-8b44bd33a92a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating: 1/17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1231377406.py:13: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating: 2/17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1231377406.py:13: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating: 3/17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1231377406.py:13: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating: 4/17\n",
            "calculating: 5/17\n",
            "calculating: 6/17\n",
            "calculating: 7/17\n",
            "calculating: 8/17\n",
            "calculating: 9/17\n",
            "calculating: 10/17\n",
            "calculating: 11/17\n",
            "calculating: 12/17\n",
            "calculating: 13/17\n",
            "calculating: 14/17\n",
            "calculating: 15/17\n",
            "calculating: 16/17\n",
            "calculating: 17/17\n",
            "16233002 0                            NaN\n",
            "ACK Flag Cnt            0.331602\n",
            "Active Max         262392.669868\n",
            "Active Mean        172878.053685\n",
            "Active Min         115466.001905\n",
            "                       ...      \n",
            "Tot Bwd Pkts            6.312703\n",
            "Tot Fwd Pkts           23.533122\n",
            "TotLen Bwd Pkts      4730.926465\n",
            "TotLen Fwd Pkts       973.035995\n",
            "URG Flag Cnt            0.041713\n",
            "Length: 80, dtype: float64 0                           NaN\n",
            "ACK Flag Cnt       4.704343e-01\n",
            "Active Max         3.310166e+06\n",
            "Active Mean        2.498837e+06\n",
            "Active Min         2.109640e+06\n",
            "                       ...     \n",
            "Tot Bwd Pkts       1.639441e+02\n",
            "Tot Fwd Pkts       1.520272e+03\n",
            "TotLen Bwd Pkts    2.344114e+05\n",
            "TotLen Fwd Pkts    6.214612e+04\n",
            "URG Flag Cnt       2.000817e-01\n",
            "Length: 80, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# get normalization stats\n",
        "UNWANTED_COLUMNS_STATS = ['Timestamp','Label', 'Flow ID', 'Dst IP', \"Src IP\"]\n",
        "\n",
        "def calcStatistics(files):\n",
        "  n_total = 0\n",
        "  mean_total = pd.Series(0.0)\n",
        "  M2 = pd.Series(0.0)\n",
        "\n",
        "  file_idx = 0\n",
        "  num_files = len(files)\n",
        "  for file in files:\n",
        "    print(f\"calculating: {file_idx+1}/{num_files}\")\n",
        "    df = pd.read_csv(file)\n",
        "    df = df.reindex(columns=CANON_COLUMN_INDEX)\n",
        "    for col in df.columns:\n",
        "      if col not in UNWANTED_COLUMNS_STATS:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').astype('float32')\n",
        "\n",
        "        if col not in mean_total:\n",
        "          mean_total[col] = 0.0\n",
        "          M2[col] = 0.0\n",
        "\n",
        "      else:\n",
        "        df = df.drop(columns=[col])\n",
        "\n",
        "    n = len(df)\n",
        "    mean = df.mean()\n",
        "    var = df.var(ddof=0)\n",
        "\n",
        "    # Welfordâ€™s algorithm for combining means and variances\n",
        "    delta = mean - mean_total\n",
        "    new_n = n_total + n\n",
        "\n",
        "    mean_total = (n_total * mean_total + n * mean) / new_n\n",
        "    M2 += var * n + (delta**2) * n_total * n / new_n\n",
        "\n",
        "    n_total = new_n\n",
        "\n",
        "    file_idx += 1\n",
        "    del df\n",
        "    gc.collect()\n",
        "\n",
        "  std = (M2 / n_total) ** 0.5\n",
        "  length = n_total\n",
        "  mean = mean_total\n",
        "  return (length, mean, std)\n",
        "\n",
        "LENGTH, MEAN, STD = calcStatistics(CSV_FILES)\n",
        "\n",
        "import json\n",
        "j = {'length': LENGTH, 'mean': MEAN.to_dict(), 'std': STD.to_dict(), 'non-stat-columns': UNWANTED_COLUMNS_STATS}\n",
        "with open('info.json', 'w') as f:\n",
        "  json.dump(j,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pCO8120Wncm",
        "outputId": "971263d1-582b-4faf-81c0-d153e3f95607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "normalizing: 1/17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-10-3407048668.py:12: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "normalizing: 2/17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-10-3407048668.py:12: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "normalizing: 3/17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-10-3407048668.py:12: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "normalizing: 4/17\n",
            "normalizing: 5/17\n",
            "normalizing: 6/17\n",
            "normalizing: 7/17\n",
            "normalizing: 8/17\n",
            "normalizing: 9/17\n",
            "normalizing: 10/17\n",
            "normalizing: 11/17\n",
            "normalizing: 12/17\n",
            "normalizing: 13/17\n",
            "normalizing: 14/17\n",
            "normalizing: 15/17\n",
            "normalizing: 16/17\n",
            "normalizing: 17/17\n",
            "['/content/normalized/normalized_02-28-2018_part1.parquet', '/content/normalized/normalized_03-01-2018_part1.parquet', '/content/normalized/normalized_02-16-2018_part1.parquet', '/content/normalized/normalized_02-15-2018_part1.parquet', '/content/normalized/normalized_02-21-2018_part1.parquet', '/content/normalized/normalized_03-02-2018_part1.parquet', '/content/normalized/normalized_02-22-2018_part1.parquet', '/content/normalized/normalized_02-20-2018_part1.parquet', '/content/normalized/normalized_02-20-2018_part2.parquet', '/content/normalized/normalized_02-20-2018_part3.parquet', '/content/normalized/normalized_02-20-2018_part4.parquet', '/content/normalized/normalized_02-20-2018_part5.parquet', '/content/normalized/normalized_02-20-2018_part6.parquet', '/content/normalized/normalized_02-20-2018_part7.parquet', '/content/normalized/normalized_02-20-2018_part8.parquet', '/content/normalized/normalized_02-14-2018_part1.parquet', '/content/normalized/normalized_02-23-2018_part1.parquet']\n"
          ]
        }
      ],
      "source": [
        "# normalize dataset and convert to parquet\n",
        "! mkdir \"normalized/\"\n",
        "\n",
        "UNWANTED_COLUMNS_NORM = ['Flow ID', 'Dst IP', \"Src IP\"]\n",
        "\n",
        "def normalize(files, mean, std):\n",
        "  pq_files = []\n",
        "\n",
        "  num_files = len(files)\n",
        "  for i,file in enumerate(CSV_FILES):\n",
        "    print(f\"normalizing: {i+1}/{num_files}\")\n",
        "    df = pd.read_csv(file)\n",
        "    df = df.reindex(columns=CANON_COLUMN_INDEX)\n",
        "    filename = os.path.splitext(os.path.basename(file))[0]\n",
        "    name = f'/content/normalized/normalized_{filename}.parquet'\n",
        "    pq_files.append(name)\n",
        "\n",
        "    for col in df.columns:\n",
        "      if col not in UNWANTED_COLUMNS_STATS:\n",
        "          df[col] = pd.to_numeric(df[col], errors='coerce').astype('float32')\n",
        "          df[col] = (df[col] - mean[col]) / std[col]\n",
        "      else:\n",
        "        if col in UNWANTED_COLUMNS_NORM:\n",
        "          df = df.drop(columns=[col])\n",
        "\n",
        "    if 'Src Port' not in df.columns:\n",
        "      df['Src Port'] = np.nan\n",
        "\n",
        "    df.to_parquet(name)\n",
        "\n",
        "    del df\n",
        "    gc.collect()\n",
        "  return pq_files\n",
        "\n",
        "PARQUET_FILES = normalize(CSV_FILES, MEAN, STD)\n",
        "print(PARQUET_FILES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47jYYjNdSD-J"
      },
      "outputs": [],
      "source": [
        "# double check columns\n",
        "base_df = pd.read_parquet(PARQUET_FILES[0])\n",
        "base = set(list(base_df.columns))\n",
        "del base_df\n",
        "gc.collect()\n",
        "\n",
        "for f in PARQUET_FILES:\n",
        "  df = pd.read_parquet(f)\n",
        "  cols = set(list(df.columns))\n",
        "  if base != cols:\n",
        "    print(f'in base: {base - cols}, in cols: {cols - base}')\n",
        "  del df\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbCuYDXuobPe",
        "outputId": "30417726-a9bb-4d83-a081-575cacfec4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "normalized/normalized_02-14-2018_part1.parquet\n",
            "normalized/normalized_02-15-2018_part1.parquet\n",
            "normalized/normalized_02-16-2018_part1.parquet\n",
            "normalized/normalized_02-20-2018_part1.parquet\n",
            "normalized/normalized_02-20-2018_part2.parquet\n",
            "normalized/normalized_02-20-2018_part3.parquet\n",
            "normalized/normalized_02-20-2018_part4.parquet\n",
            "normalized/normalized_02-20-2018_part5.parquet\n",
            "normalized/normalized_02-20-2018_part6.parquet\n",
            "normalized/normalized_02-20-2018_part7.parquet\n",
            "normalized/normalized_02-20-2018_part8.parquet\n",
            "normalized/normalized_02-21-2018_part1.parquet\n",
            "normalized/normalized_02-22-2018_part1.parquet\n",
            "normalized/normalized_02-23-2018_part1.parquet\n",
            "normalized/normalized_02-28-2018_part1.parquet\n",
            "normalized/normalized_03-01-2018_part1.parquet\n",
            "normalized/normalized_03-02-2018_part1.parquet\n",
            "info.json\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "! tar -czvf normalized-ids2018-parquet.tar.gz normalized/*.parquet info.json\n",
        "# ! cp normalized-ids2018.tar.gz drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoJbjdh5cEuo"
      },
      "outputs": [],
      "source": [
        "! rm ./*.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
